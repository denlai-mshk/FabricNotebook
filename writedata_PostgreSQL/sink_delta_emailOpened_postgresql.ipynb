{"cells":[{"cell_type":"code","execution_count":null,"id":"f8c292f1-8a39-4463-9b0b-667b4f103308","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# change to use your AKV endpoint\n","AKV_ENDPOINT = \"https://xxx.vault.azure.net/\"\n","\n","#cut off date the data ingestion CIJ data, any record before this date will be ignored\n","cutoffdate = \"2024-05-09 09:46:55\"\n","\n","# Define the JDBC URL for your PostgrSQL \n","jdbc_url = \"jdbc:postgresql://xxx.postgres.database.azure.com:5432/fabricoutput\""]},{"cell_type":"code","execution_count":null,"id":"6f7d1ce5-be3b-4a5c-823c-9350886592d3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","#customer should setup Azure Key Vault and store secret in AKV instead of hardcoding here\n","#Please refer to this guildeline https://www.datasarva.com/fabric-notebook-azurekeyvault/\n","\n","os.environ[\"AZURE_CLIENT_ID\"] = mssparkutils.credentials.getSecret(AKV_ENDPOINT, 'AZURE-CLIENT-ID')\n","os.environ[\"AZURE_TENANT_ID\"] = mssparkutils.credentials.getSecret(AKV_ENDPOINT, 'AZURE-TENANT-ID')\n","os.environ[\"AZURE_CLIENT_SECRET\"] = mssparkutils.credentials.getSecret(AKV_ENDPOINT, 'AZURE-CLIENT-SECRET')\n"]},{"cell_type":"code","execution_count":null,"id":"88d29ff5-6f9e-4f7c-8d8c-6295273d153c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType\n","from notebookutils import mssparkutils\n","\n","#get workspace name\n","workspaceidty = mssparkutils.env.getWorkspaceName()\n","\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"PostgreSQL Example\") \\\n","    .config(\"spark.jars\", \"/path/to/postgresql-42.2.18.jar\") \\\n","    .getOrCreate()\n","\n","# Define the JDBC URL and connection properties\n","jdbc_url = \"jdbc:postgresql://denlaipostgresqlwestus3.postgres.database.azure.com:5432/fabricoutput\"\n","\n","from azure.identity import DefaultAzureCredential\n","\n","credential = DefaultAzureCredential()\n","\n","token = credential.get_token(\"https://ossrdbms-aad.database.windows.net/.default\").token\n","\n","connection_properties = {\n","    \"user\": workspaceidty,\n","    \"password\": token,\n","    \"driver\": \"org.postgresql.Driver\"\n","}\n","\n","#print(\"token = \", token)"]},{"cell_type":"code","execution_count":null,"id":"f5289322-1569-403e-8def-5de83dabb4c9","metadata":{"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":true}},"outputs":[],"source":["# just for debug\n","# if you want to reset the lasttimestamp stored in table last_ts_opened, then unfreeze this cell \n","schema = StructType([\n","    StructField(\"lasttimestamp\", StringType(), True)\n","])\n","\n","# Create the DataFrame\n","data = [(cutoffdate,)]\n","write_ts_df = spark.createDataFrame(data, schema)\n","\n","# Write DataFrame to PostgreSQL\n","write_ts_df.write \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", jdbc_url) \\\n","    .option(\"dbtable\", \"public.last_ts_opened\") \\\n","    .options(**connection_properties) \\\n","    .mode(\"overwrite\") \\\n","    .save()"]},{"cell_type":"code","execution_count":null,"id":"f4369f1c-c035-4742-8331-5e329423ee7f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["\n","# Query to check if the table exists\n","query = \"\"\"\n","SELECT EXISTS (\n","    SELECT 1\n","    FROM information_schema.tables \n","    WHERE table_schema = 'public'\n","    AND table_name = 'last_ts_opened'\n",") AS table_exists\n","\"\"\"\n","\n","# Execute the query\n","dfchecktbl = spark.read \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", jdbc_url) \\\n","    .option(\"query\", query) \\\n","    .options(**connection_properties) \\\n","    .load()\n","\n","# Check if the table exists\n","table_exists = dfchecktbl.collect()[0][\"table_exists\"]\n","\n","if not table_exists:\n","    print(\"table_exists = NO\", )\n","    # Define the schema\n","    schema = StructType([\n","        StructField(\"lasttimestamp\", StringType(), True)\n","    ])\n","\n","    # Create the DataFrame\n","    data = [(cutoffdate,)]\n","    write_ts_df = spark.createDataFrame(data, schema)\n","\n","    # Write DataFrame to PostgreSQL\n","    write_ts_df.write \\\n","      .format(\"jdbc\") \\\n","      .option(\"url\", jdbc_url) \\\n","      .option(\"dbtable\", \"public.last_ts_opened\") \\\n","      .options(**connection_properties) \\\n","      .mode(\"overwrite\") \\\n","      .save()\n","\n","    last_ts = cutoffdate\n","    print(\"last_ts = \", last_ts)\n","else:\n","    print(\"table_exists = YES\", )\n","    # Read the table\n","    load_ts_df = spark.read \\\n","        .format(\"jdbc\") \\\n","        .option(\"url\", jdbc_url) \\\n","        .option(\"dbtable\", \"public.last_ts_opened\") \\\n","        .options(**connection_properties) \\\n","        .load()\n","\n","    # Show the first row value\n","    first_row = load_ts_df.first()\n","    last_ts = first_row[\"lasttimestamp\"]\n","    print(\"last_ts = \", last_ts)\n","    load_ts_df.show()"]},{"cell_type":"code","execution_count":null,"id":"777d723e-1f41-45eb-b326-b479f84de79b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col, max\n","\n","# Filter the DataFrame to include only records with Timestamp > '2024-05-09 09:46:55'\n","filtered_df = spark.read.parquet(\"Files/EmailOpened\").filter(col(\"Timestamp\") > last_ts)\n","\n","# Check if the DataFrame is empty\n","if filtered_df.count() == 0:\n","    print(\"filtered_df is empty. Exiting notebook with positive finish state.\")\n","    mssparkutils.notebook.exit(\"Success, filtered_df is empty\")\n","\n","# Deduplicate the DataFrame\n","print(\"before dedup count = \", filtered_df.count())\n","dup_filtered_df = filtered_df.dropDuplicates()\n","print(\"after dedup count = \", dup_filtered_df.count())\n","\n","# Get the maximum value of the Timestamp column\n","if dup_filtered_df.count() > 0:\n","    newlast_ts = dup_filtered_df.agg(max(col(\"Timestamp\"))).collect()[0][0]\n","    print(\"Last Timestamp: \", newlast_ts)\n","else:\n","    print(\"dup_filtered_df DataFrame is empty after deduplication.\")\n","    mssparkutils.notebook.exit(\"Success, dup_filtered_df is empty\")    \n","\n","if not isinstance(newlast_ts, str):\n","    newlast_ts = str(newlast_ts)\n","\n","print(\"newlast_ts\", newlast_ts)\n","\n","# Show the filtered DataFrame\n","#dup_filtered_df.show()"]},{"cell_type":"code","execution_count":null,"id":"f6cd7f58-3fde-4844-8b72-ce17464de0db","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Sample code for manipulating columns\n","\n","# Get a list of all column names\n","#columns = dup_filtered_df.columns\n","#print(\"Columns: \", columns)\n","\n","# Rename columns\n","#df = dup_filtered_df.withColumnRenamed(\"old_name1\", \"new_name1\")    \n","\n","# Select only the \"ActivityId\" and \"Timestamp\" columns\n","#selected_df = dup_filtered_df.select(\"ActivityId\", \"Timestamp\")\n","\n","# Show the DataFrame with only the selected columns\n","#selected_df.show()"]},{"cell_type":"code","execution_count":null,"id":"3f1b6f93-fdef-41a1-b48c-acbe0efff89b","metadata":{"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":true}},"outputs":[],"source":["# Just for debug\n","# Print the count\n","print(\"filtered_record_count:\", filtered_df.count())\n","print(\"dedup_record_count:\", dup_filtered_df.count())\n"]},{"cell_type":"code","execution_count":null,"id":"fb4f5f21-758e-49ed-8421-7497a4f2988f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Write DataFrame to PostgreSQL\n","dup_filtered_df.write \\\n","  .format(\"jdbc\") \\\n","  .option(\"url\", jdbc_url) \\\n","  .option(\"dbtable\", \"public.emailopened\") \\\n","  .options(**connection_properties) \\\n","  .mode(\"append\") \\\n","  .save()\n"]},{"cell_type":"code","execution_count":null,"id":"47c7353d-ce0a-427a-9371-7f18f5b3b1ce","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Write new timestamp \n","\n","schema = StructType([\n","    StructField(\"lasttimestamp\", StringType(), True)\n","])\n","print(\"newlast_ts\", newlast_ts)\n","data = [(newlast_ts,)]\n","newts_df = spark.createDataFrame(data, schema)\n","\n","\n","newts_df.write \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", jdbc_url) \\\n","    .option(\"dbtable\", \"public.last_ts_opened\") \\\n","    .options(**connection_properties) \\\n","    .mode(\"overwrite\") \\\n","    .save()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"70a5366e-58b1-4b7b-9f57-0995e30f195b","metadata":{"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":true}},"outputs":[],"source":["# just for debug\n","# Validate the records in db\n","# Read the table back from PostgreSQL\n","read_updated_df = spark.read \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", jdbc_url) \\\n","    .option(\"dbtable\", \"public.emailopened\") \\\n","    .options(**connection_properties) \\\n","    .load()\n","\n","# Get the count of records in the original DataFrame\n","dedup_count = dup_filtered_df.count()\n","\n","# Get the count of records in the table read from PostgreSQL\n","added_count = read_updated_df.count()\n","\n","# Validate the counts\n","if dedup_count == added_count:\n","    print(\"Validation successful: The number of records matches.\")\n","else:\n","    print(f\"Validation failed: Dedupfiltered count = {dedup_count}, added_count = {added_count}\")"]},{"cell_type":"code","execution_count":null,"id":"da934144-3688-4f59-814e-f0621d64b79f","metadata":{"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":true}},"outputs":[],"source":["# just for debug\n","# Validate the timestamp records in db\n","read_ts_df = spark.read \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", jdbc_url) \\\n","    .option(\"dbtable\", \"public.last_ts_opened\") \\\n","    .options(**connection_properties) \\\n","    .load()\n","\n","\n","# Show the first row value\n","first_row = read_ts_df.first()\n","newlast_ts = first_row[\"lasttimestamp\"]\n","print(\"newlast_ts = \", newlast_ts)\n"]},{"cell_type":"code","execution_count":null,"id":"864089de-0257-4bad-953c-ecead108309a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#If Fabric Capacity is F2/F4 and you have more than one concurrently session\n","# you need to unremark mssparkutils.session.stop() this line to let Fabric terminate one session before start another one\n","# You need to wait for new session startup if you stop the session\n","\n","# release session resources\n","# mssparkutils.session.stop()"]}],"metadata":{"dependencies":{"lakehouse":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
