{"cells":[{"cell_type":"code","source":["# Define the output file path and default timestamp path\n","ts_file_path = \"Files/adls2_fabricoutput/ts_emailclicked\"\n","output_file_path = \"Files/adls2_fabricoutput/emailclicked\"\n","\n","#cut off date the data ingestion CIJ data, any record before this date will be ignored\n","cutoffdate = \"2024-05-09 09:46:55\"\n","\n","# Define the output format (either 'csv' or 'json')\n","#output_format = 'csv'  # Change this to 'json' if you want JSON output\n","output_format = 'json'  # Change this to 'json' if you want JSON output"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa6cdc82-dc47-4122-99d8-0508055045cb"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","from pyspark.sql import SparkSession\n","\n","# Create a Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"ADLS2 Example\") \\\n","    .getOrCreate()\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"88d29ff5-6f9e-4f7c-8d8c-6295273d153c"},{"cell_type":"code","source":["from pyspark.sql.utils import AnalysisException\n","from datetime import datetime\n","from pyspark.sql.types import StructType, StructField, StringType\n","\n","\n","# Function to check if the file exists\n","def file_exists(path):\n","    try:\n","        # Try to read the file\n","        ts_df = spark.read.text(path)\n","        return ts_df\n","    except AnalysisException:\n","        # If the file does not exist or there's a read failure\n","        return None\n","    \n","\n","\n","# Function to write or read timestamp\n","def writeReadTS(ts_file_path, cutoffdate):\n","    ts_df = file_exists(ts_file_path)\n","    if ts_df is None:\n","        # Create a DataFrame with the default timestamp value\n","        data = [(cutoffdate,)]\n","        ts_df = spark.createDataFrame(data, [\"timestamp\"])\n","        \n","        # Write the DataFrame to a single TXT file in ADLS2\n","        ts_df.coalesce(1).write.mode(\"overwrite\").text(ts_file_path)\n","        \n","        print(f\"File created with default timestamp: {cutoffdate}\")\n","    else:\n","        print(\"File already exists or read successfully.\")\n","    \n","    return ts_df\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"056d5c34-c890-4658-af7d-abac9d4aa1ce"},{"cell_type":"code","source":["# Get last stored timestamp or create a default one for 1st time\n","ts_df = writeReadTS(ts_file_path, cutoffdate)\n","\n","# Define the schema for reading the text file\n","schema = StructType([StructField(\"timestamp\", StringType(), True)])\n","\n","# Read the text file back into a DataFrame with the specified schema\n","ts_df = spark.read.schema(schema).text(ts_file_path)\n","\n","# Collect the DataFrame to a list of Row objects\n","rows = ts_df.collect()\n","\n","# Extract the timestamp value from the first row\n","last_ts = rows[0][\"timestamp\"]\n","\n","print(f\"last_ts = {last_ts}\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"235fced1-f8e0-48ca-ab2a-50a9d36aa6b6"},{"cell_type":"code","source":["from pyspark.sql.functions import col, max\n","\n","# Filter the DataFrame to include only records with Timestamp > '2024-05-09 09:46:55'\n","filtered_df = spark.read.parquet(\"Files/EmailClicked\").filter(col(\"Timestamp\") > last_ts)\n","\n","# Check if the DataFrame is empty\n","if filtered_df.count() == 0:\n","    print(\"filtered_df is empty. Exiting notebook with positive finish state.\")\n","    mssparkutils.notebook.exit(\"Success, filtered_df is empty\")\n","\n","# Deduplicate the DataFrame\n","print(\"before dedup count = \", filtered_df.count())\n","dup_filtered_df = filtered_df.dropDuplicates()\n","print(\"after dedup count = \", dup_filtered_df.count())\n","\n","# Get the maximum value of the Timestamp column\n","if dup_filtered_df.count() > 0:\n","    newlast_ts = dup_filtered_df.agg(max(col(\"Timestamp\"))).collect()[0][0]\n","    print(\"Last Timestamp: \", newlast_ts)\n","else:\n","    print(\"dup_filtered_df DataFrame is empty after deduplication.\")\n","    mssparkutils.notebook.exit(\"Success, dup_filtered_df is empty\")    \n","\n","if not isinstance(newlast_ts, str):\n","    newlast_ts = str(newlast_ts)\n","\n","print(\"newlast_ts\", newlast_ts)\n","\n","# Show the filtered DataFrame\n","#dup_filtered_df.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f6fde865-ac79-41be-98d8-a3a06d279cfe"},{"cell_type":"code","source":["#Sample code for manipulating columns\n","\n","# Get a list of all column names\n","#columns = dup_filtered_df.columns\n","#print(\"Columns: \", columns)\n","\n","# Rename columns\n","#df = dup_filtered_df.withColumnRenamed(\"old_name1\", \"new_name1\")    \n","\n","# Select only the \"ActivityId\" and \"Timestamp\" columns\n","#selected_df = dup_filtered_df.select(\"ActivityId\", \"Timestamp\")\n","\n","# Show the DataFrame with only the selected columns\n","#selected_df.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9d4e999-4f4d-45e8-a9c5-7f7648f920b6"},{"cell_type":"code","source":["# Just for debug\n","# Print the count\n","print(\"filtered_record_count:\", filtered_df.count())\n","print(\"dedup_record_count:\", dup_filtered_df.count())\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f1b6f93-fdef-41a1-b48c-acbe0efff89b"},{"cell_type":"code","source":["# append record to ADLS2\n","from datetime import datetime\n","\n","# Get the current timestamp\n","current_timestamp = datetime.now()\n","\n","# Format the timestamp for the folder structure\n","year = current_timestamp.strftime(\"%Y\")\n","month = current_timestamp.strftime(\"%m\")\n","day = current_timestamp.strftime(\"%d\")\n","\n","# Define the output path with the formatted timestamp\n","output_path = f\"{output_file_path}/{year}/{month}/{day}/\"\n","\n","# Export the filtered DataFrame to the specified format with the dynamic filename\n","if output_format == 'csv':\n","    dup_filtered_df.coalesce(1).write.mode(\"append\").csv(output_path, header=True)\n","elif output_format == 'json':\n","    dup_filtered_df.coalesce(1).write.mode(\"append\").json(output_path)\n","\n","# Print the output path\n","print(\"Output path:\", output_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2f7b0d4-dd82-4fd0-b9e0-49dcf33327ec"},{"cell_type":"code","source":["# save the new timestamp\n","\n","data = [(newlast_ts,)]\n","newts_df = spark.createDataFrame(data, [\"timestamp\"])\n","\n","# Write the DataFrame to a single TXT file in ADLS2\n","newts_df.coalesce(1).write.mode(\"overwrite\").text(ts_file_path)\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59741510-270f-4ae6-92cc-b5ed789ddb0c"},{"cell_type":"code","source":["# just for debug\n","# Validate the records in db\n","# Load the exported CSV into another DataFrame\n","if output_format == 'csv':\n","    loaded_df = spark.read.csv(output_path, header=True, inferSchema=True)\n","elif output_format == 'json':\n","    loaded_df = spark.read.json(output_path)\n","\n","\n","# Validate the count\n","if loaded_df.count() == dup_filtered_df.count():\n","    print(\"Validation successful! Record counts match.\")\n","else:\n","    print(f\"Validation failed! Filtered record count: {dup_filtered_df.count()}, Loaded record count: {loaded_df.count()}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"9a22466f-a21f-4be8-852a-3d0ce508fca8"},{"cell_type":"code","source":["# just for debug\n","# Validate the timestamp records in db\n","# Define the schema for reading the text file\n","schema = StructType([StructField(\"timestamp\", StringType(), True)])\n","\n","# Read the text file back into a DataFrame with the specified schema\n","newts_df = spark.read.schema(schema).text(ts_file_path)\n","\n","# Collect the DataFrame to a list of Row objects\n","rows = newts_df.collect()\n","\n","# Extract the timestamp value from the first row\n","read_newlast_ts = rows[0][\"timestamp\"]\n","\n","print(f\"read newlast_ts= {read_newlast_ts}\")\n","\n","print(f\"newlast_ts = {newlast_ts}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"f4d0f2fe-3f47-43fb-a40f-63c4523d362f"},{"cell_type":"code","source":["#If Fabric Capacity is F2/F4 and you have more than one concurrently session\n","# you need to unremark mssparkutils.session.stop() this line to let Fabric terminate one session before start another one\n","# You need to wait for new session startup if you stop the session\n","\n","# release session resources\n","# mssparkutils.session.stop()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2fe4c96-ce05-429a-93a3-2f2f17a8c52b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}